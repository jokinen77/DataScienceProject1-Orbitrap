{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Project 1 -  Improve the mass accuracy of spectra measured by Orbitrap mass spectrometers (orbitrap)\n",
    "\n",
    "**Client:** Atmospheric Physical Chemistry group, INAR, University of Helsinki\n",
    "\n",
    "**Description:** Motivation: When using a mass spectrometer, the measured mass usually shifts from its true mass. Hence, a mass calibration is an important procedure before allocating chemical formulae to the measured masses. A good mass calibration may greatly reduce the efforts of further analysis and increase the reliability of the results. Goals: Improve the mass calibration procedure for Orbitrap raw data, and perhaps for data measured by other mass spectrometers, e.g, TOF-MS Main tasks: 1) Test several fitting function for mass correction, and recommend one or a few that works best. 2) Test the performance of different parameters for mass correction, e.g., number of mass of the species for calibration.\n",
    "\n",
    "**Data and tools:** Data: raw spectrum data measured by Orbitrap mass spectrometer. Tools: a) Orbitool, provided by the client. Orbitool will be used for reading the raw data and remove the noise, i.e., prepare the data for this analysis; b) Any programming language, which will be used to investigate this mass calibration problem.\n",
    "\n",
    "## Environment\n",
    "\n",
    "The environment should have the dependencies to run Orbitool and this notebook.\n",
    "\n",
    "```bash\n",
    "# Import environment\n",
    "conda env create -f environment.yml\n",
    "\n",
    "# Import kernel to jupyter\n",
    "ipython kernel install --user --name=orbitool\n",
    "\n",
    "# Export the environment to file\n",
    "conda env export --no-builds > environment.yml\n",
    "```\n",
    "\n",
    "## Notebook practices\n",
    "\n",
    "* Clean outputs before commiting!\n",
    "\n",
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.optimize import leastsq\n",
    "from pyteomics import mass\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "### Read the spectrum data\n",
    "\n",
    "* Put the directory of spectrum CSV files to ```spectrum_data_directory``` variable.\n",
    "    * If the begining of the CSV files contains some time information, then set ```contains_time_data``` variable to ```True```.\n",
    "* Put the path of the peak list file to ```peak_list_file``` variable.\n",
    "* If you just want to test the code, then you can speed up the code by setting the size of spectrum sample to ```random_sample_size``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectraRoot = \"\"\n",
    "peaklistRoot = \"\"\n",
    "spectrum_data_directory = spectraRoot + \"data/First Large Deviation file-20210415T191813Z-001/First Large Deviation file/2 mins\"\n",
    "contains_time_data = False #Spectrum files contains time data in forst 3 rows?\n",
    "peak_list_file = peaklistRoot + \"peak list/peaklist_1e5_background.csv\"\n",
    "random_sample_size = 5 #Take sample spectrums to speed up things. If 'None' then uses all the spectrums\n",
    "\n",
    "spectrum_data_files = []\n",
    "time_data = []\n",
    "spectrums = []\n",
    "\n",
    "# Get data files\n",
    "for file in [f for f in listdir(spectrum_data_directory) if isfile(join(spectrum_data_directory, f))]:\n",
    "    path_to_file = join(spectrum_data_directory, file)\n",
    "    #print(path_to_file)\n",
    "    if file.endswith(\".csv\"):\n",
    "        spectrum_data_files.append(path_to_file)\n",
    "\n",
    "# Take sample spectrums to speed up things\n",
    "if random_sample_size:\n",
    "    spectrum_data_files = random.sample(spectrum_data_files, min(random_sample_size, len(spectrum_data_files)))\n",
    "\n",
    "# Read the spectrum files\n",
    "for file in spectrum_data_files:\n",
    "    if contains_time_data:\n",
    "        time_data.append(pd.read_csv(file)[:2])\n",
    "        spectrums.append(pd.read_csv(file, skiprows = 3).sort_values(by=['mz']))\n",
    "    else:\n",
    "        spectrums.append(pd.read_csv(file))\n",
    "\n",
    "peak_list = dict(pd.read_csv(peak_list_file).values)\n",
    "\n",
    "# Lookup\n",
    "spectrums[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot  the spectrums\n",
    "\n",
    "* Plot all the spectrums with actual ions (from peak list file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# seaborn version should be at least 0.11!\n",
    "print(sns.__version__)\n",
    "\n",
    "# Concat files to same data frame with 'spectrum' separator column\n",
    "tmp = pd.concat(spectrums, keys=range(len(spectrum_data_files)), names=[\"spectrum\"])\n",
    "\n",
    "# Plot the spectrums\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.lineplot(data=tmp, ax=ax, x=\"mz\", y=\"intensity\", hue=\"spectrum\", palette=\"tab10\")\n",
    "ax.legend([],[], frameon=False)\n",
    "ax.ticklabel_format(useOffset=False)\n",
    "\n",
    "# Plot the actual masses. Line height is taken from max intensity with +-eps interval.\n",
    "eps = 0.0001\n",
    "for formula, mz in peak_list.items():\n",
    "    max_intensity = max([x for x in tmp.loc[(tmp[\"mz\"] > mz - eps) & (tmp[\"mz\"] < mz + eps)].intensity.values] + [0.0])\n",
    "    plt.plot((mz,mz), (0.0,max_intensity), linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect peaks\n",
    "\n",
    "* Detect peaks\n",
    "* Calculate/get some information about the peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peak_indices(data):\n",
    "    '''\n",
    "    Logic is same as in Koli's implementation. Tested that produces same results with spectrums in\n",
    "    \"data/CI-orbi_20201117165601 folder (first smal deviation file)/1 min\" folder.\n",
    "    '''\n",
    "    peak_start_indices = [0] #Initialize with first start index\n",
    "    peak_end_indices = []\n",
    "    for i, row in data.iterrows():\n",
    "        if i == 0:\n",
    "            continue;\n",
    "        if data.iloc[i][\"intensity\"] == 0.0 and data.iloc[i - 1][\"intensity\"] == 0.0:\n",
    "            peak_start_indices.append(i)\n",
    "            peak_end_indices.append(i - 1)\n",
    "    peak_end_indices.append(len(data) - 1) #Finalize with last end index\n",
    "    return np.column_stack((peak_start_indices, peak_end_indices))\n",
    "\n",
    "peak_informations = []\n",
    "for spectrum in spectrums:\n",
    "    peak_information = []\n",
    "    for start, end in find_peak_indices(spectrum):\n",
    "        data = spectrum.iloc[start:(end+1)]\n",
    "        max_intensity = data[\"intensity\"].max()\n",
    "        width = data[\"mz\"].max() - data[\"mz\"].min()\n",
    "        average_mz = np.average(data[\"mz\"], weights=data[\"intensity\"])\n",
    "        peak_information.append({\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"max_intensity\": max_intensity,\n",
    "            \"average_mz\": average_mz,\n",
    "            \"width\": width\n",
    "        })\n",
    "    peak_informations.append(pd.DataFrame(peak_information))\n",
    "    \n",
    "peak_informations[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle 'double peaks'\n",
    "\n",
    "* Converts 'double peaks' to separate peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Gaussian curve to peaks\n",
    "\n",
    "* Fit Gaussian curve to peaks and get fitted means.\n",
    "* Set the wanted resolution to ```default_resolution``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_resolution = 280000\n",
    "\n",
    "# x: data; a: height; x0: position; c: sigma or width\n",
    "gauss  = lambda x, a, mu, sigma: a*np.exp(-(x-mu)**2/(2*sigma**2))\n",
    "\n",
    "def fit_gaussian(peak, resolution=default_resolution, show=False, use_rolling = None):\n",
    "    '''\n",
    "    returns: a: 'height'; mu: 'position'; sigma: 'width'\n",
    "    '''\n",
    "    intensity = peak[\"intensity\"] if not use_rolling else peak[\"intensity\"].rolling(use_rolling, center=True, min_periods=1).mean()\n",
    "    mu = np.average(peak[\"mz\"], weights=intensity)\n",
    "    sigma = mu/(resolution*2*np.sqrt(2*np.log(2)))\n",
    "    errfunc  = lambda p, x, y: (y - gauss(x, p[0], p[1], sigma))\n",
    "    init  = [intensity.max(), mu]\n",
    "    out = leastsq(errfunc, init, args=(peak[\"mz\"], intensity), maxfev = 200)\n",
    "    c = out[0]\n",
    "    if show:\n",
    "        x = np.linspace(peak[\"mz\"].min(), peak[\"mz\"].max(), 1000)\n",
    "        fig, ax = plt.subplots(figsize=(8,6))\n",
    "        ax.ticklabel_format(useOffset=False)\n",
    "        plt.plot(peak[\"mz\"], peak[\"intensity\"], \"b\")\n",
    "        if use_rolling:\n",
    "            plt.plot(peak[\"mz\"], intensity, \"b\", linestyle=\"--\")\n",
    "        plt.plot(x, gauss(x, c[0], c[1], sigma), \"g\")\n",
    "        plt.plot((c[1],c[1]), (0.0,c[0]), \"g\", linestyle=\"--\")\n",
    "    return c[0], c[1], sigma\n",
    "\n",
    "# Test Fitting\n",
    "n_spectrum = 0\n",
    "n_peak = 163\n",
    "\n",
    "spectrum = spectrums[n_spectrum]\n",
    "peak_information = peak_informations[n_spectrum]\n",
    "peak = peak_information.iloc[n_peak]\n",
    "data = spectrum.iloc[int(peak[\"start\"]):int(peak[\"end\"] + 1)]\n",
    "a, mu, sigma = fit_gaussian(data, show=True, use_rolling = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'observed_mz' column\n",
    "for spectrum, peak_information in zip(spectrums, peak_informations):\n",
    "    fitted_means = []\n",
    "    for i, peak in peak_information.iterrows():\n",
    "        data = spectrum.iloc[int(peak[\"start\"]):int(peak[\"end\"] + 1)]\n",
    "        a, mu, sigma = fit_gaussian(data)\n",
    "        fitted_means.append(mu)\n",
    "    peak_information[\"observed_mz\"] = fitted_means\n",
    "    \n",
    "peak_informations[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify ions\n",
    "\n",
    "* Associate every ion from the ```peak_list``` to closest detected peak if there is detected peaks closer than ```theta```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_peaks(peak_information, peak_list, theta = 0.01):\n",
    "    peak_information[\"formula\"] = None # For strings\n",
    "    peak_information[\"formula_mz\"] = np.nan # For floats\n",
    "    for key in peak_list.keys():\n",
    "        formula = key\n",
    "        true_mz = peak_list[key]\n",
    "        closest = peak_information[np.abs(peak_information[\"observed_mz\"] - true_mz) < theta]\n",
    "        if len(closest) > 0:\n",
    "            closest = closest.iloc[(closest['observed_mz'] - true_mz).abs().argsort()[:1]]\n",
    "            #print(\"True: {}; Observed: {}; Index: {}\".format(true_mz, closest[\"observed\"].values[0], closest.index.values[0]))\n",
    "            peak_information.at[closest.index.values[0], \"formula\"] = formula\n",
    "            peak_information.at[closest.index.values[0], \"formula_mz\"] = true_mz\n",
    "    peak_information[\"error\"] =  peak_information[\"formula_mz\"] - peak_information[\"observed_mz\"]\n",
    "\n",
    "# Identify peaks\n",
    "for peak_information in peak_informations:\n",
    "    identify_peaks(peak_information, peak_list)\n",
    "\n",
    "# Lookup\n",
    "peak_informations[0].loc[peak_informations[0][\"formula\"].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty of mass\n",
    "\n",
    "* Analyse uncertainty of mass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculateMassUncertainty function takes a dataframe containing a processed spectrum with the following columns: formula, mz, observerd.\n",
    "It returns a dictionary with the uncertainty for each element calculated as the average of all the uncertainties for wich the element was present.\n",
    "By default it will take perform a weighted averged based on the number of elements in the compounds/ions.\n",
    "If weighted is set to False, it will only average the uncertainty based solely on the presence of the element.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "def calculateMassUncertainty(peak_information, weighted=True, dfOutput=True, show=False):\n",
    "    elements = {}\n",
    "    for index, row in peak_information.iterrows():\n",
    "        ion = row[\"formula\"]\n",
    "        if not ion:\n",
    "            continue;\n",
    "        ion = ion if ion[-1] != '-' else ion[:-1]\n",
    "        tmp = mass.Composition(formula=ion)\n",
    "        v = row[\"error\"]\n",
    "        total = sum(tmp.values())\n",
    "        for e in tmp.keys():\n",
    "            f = 1\n",
    "            if weighted:\n",
    "                f = tmp[e] / total\n",
    "            if e not in elements:\n",
    "                elements[e] = [v*f]\n",
    "            else:\n",
    "                elements[e].append(v*f)\n",
    "    for e in elements.keys():\n",
    "        elements[e] = sum(elements[e]) / len(elements[e])\n",
    "    if show:\n",
    "        keys = elements.keys()\n",
    "        values = elements.values()\n",
    "        plt.figure(1)\n",
    "        plt.bar(keys, values)\n",
    "    if dfOutput:\n",
    "        df = pd.DataFrame(elements.items(), columns=['Element', 'Uncertainty'])\n",
    "        return df\n",
    "    else:\n",
    "        return elements\n",
    "    \n",
    "tmp = pd.concat(peak_informations, keys=range(len(spectrum_data_files)), names=[\"spectrum\"])\n",
    "calculateMassUncertainty(tmp, True, True,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit linear/polynomial regression model\n",
    "\n",
    "* Fits linear/polynomial model to ```error``` column.\n",
    "* ```MultiPolyModel``` is collection of polynomial models with different degrees (can also be used only with single degree). Prediction result is averaged result from the models (can be weighted).\n",
    "\n",
    "### Useful links\n",
    "\n",
    "* https://realpython.com/linear-regression-in-python/\n",
    "* https://towardsdatascience.com/polynomial-regression-which-python-package-to-use-78a09b0ac87b\n",
    "* https://stackoverflow.com/questions/34373606/scikit-learn-coefficients-polynomialfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "class MultiPolyModel:\n",
    "    '''\n",
    "    Fits multiple polynomial models with different degrees. Prediction value is average value of \n",
    "    predicted values from different models (weigths can be used).\n",
    "    \n",
    "    If 'max_degree = min_degree' then this is just simple single polynomial model with degree of \n",
    "    'max_degree' (or 'max_degree').\n",
    "    '''\n",
    "    degrees = None\n",
    "    weights = None\n",
    "    models = None\n",
    "    \n",
    "    debug = False\n",
    "    \n",
    "    @classmethod\n",
    "    def __init__(self, degrees, weights = None, debug = False):\n",
    "        '''\n",
    "        Initializes model.\n",
    "        \n",
    "        Arguments:\n",
    "            min_degree: Minium degree of model (first model).\n",
    "            max_degree: Maximum degree of model (last model).\n",
    "            weight: None, Integer, or List\n",
    "                None: Weights are degrees in inversed order (exmpl. degrees: [1,2,3] then weights: [3,2,1]).\n",
    "                Integer: All models have same weight (value doesn't matter).\n",
    "                List: Weights (size should be amout of models i.e. 'max_degree-min_degree+1').\n",
    "        '''\n",
    "        self.models = []\n",
    "        self.degrees = np.array(degrees)\n",
    "        self.debug = debug\n",
    "        \n",
    "        if weights and type(weights) == int:\n",
    "            self.weights = np.repeat(weights, len(self.degrees))\n",
    "        elif weights and type(weights) == list:\n",
    "            self.weights = np.array(weights)\n",
    "        else:\n",
    "            self.weights = 1 / self.degrees\n",
    "            \n",
    "        if self.debug:\n",
    "            print(\"Degrees: {}; Weights: {}\".format(self.degrees, self.weights))\n",
    "    \n",
    "    @classmethod\n",
    "    def fit(self, x, y, show=False, color=\"r\", ax = None):\n",
    "        for degree in self.degrees:\n",
    "            self.models.append(fit_poly_reg_model(x, y, degree = degree))\n",
    "        if show:\n",
    "            x_plot = np.linspace(min(x), max(x), 1000)\n",
    "            preds = self.predict(x_plot)\n",
    "            sns.set_theme(style=\"whitegrid\")\n",
    "            if not ax:\n",
    "                fig, ax = plt.subplots(figsize=(8,6))\n",
    "            sns.scatterplot(x=x, y=y, ax=ax, color=color)\n",
    "            plt.plot(x_plot, preds, linestyle=\"-\", color=color)\n",
    "    \n",
    "    @classmethod\n",
    "    def predict(self, x):\n",
    "        results = []\n",
    "        for model in self.models:\n",
    "            results.append(model.predict(x.reshape(-1, 1)))\n",
    "        return np.average(np.array(results), axis=0, weights = self.weights)\n",
    "    \n",
    "    @staticmethod\n",
    "    def fit_poly_reg_model(x, y, degree, show = False, ax = None, color=\"r\"):\n",
    "        poly_features = PolynomialFeatures(degree)\n",
    "        model = make_pipeline(poly_features, LinearRegression()).fit(x.reshape(-1, 1), y)\n",
    "        if show:\n",
    "            print(\"Coefficients: {}\".format(model.steps[1][1].coef_[1:]), \"Independent: {}\".format(model.steps[1][1].intercept_))\n",
    "            preds = model.predict(x.reshape(-1, 1))\n",
    "            sns.set_theme(style=\"whitegrid\")\n",
    "            if not ax:\n",
    "                fig, ax = plt.subplots(figsize=(8,6))\n",
    "            sns.scatterplot(x=x, y=y, ax=ax, color=color)\n",
    "            plt.plot(x, preds, linestyle=\"-\", color=color)\n",
    "        return model\n",
    "        \n",
    "    \n",
    "\n",
    "peak_information = peak_informations[1]\n",
    "peak_information = peak_information[peak_information[\"error\"].notnull()]\n",
    "\n",
    "# TODO: better peak selection\n",
    "train_data = peak_information[peak_information[\"max_intensity\"] > 10000]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.scatterplot(x=peak_information[\"observed_mz\"].values, y=peak_information[\"error\"].values*-1, ax=ax)\n",
    "model = MultiPolyModel([1,2,3,4,5], weights = 1, debug = True)\n",
    "model.fit(train_data[\"observed_mz\"].values, train_data[\"error\"].values*-1, show = True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak selection methods\n",
    "\n",
    "### Method 1: Select best n peaks\n",
    "\n",
    "This first idea is to select a set amount n of the best peaks to lead the calibration process. It is possible to set a treshold to specifiy the minimal value of error accepted.  \n",
    "The threshold criteria is secondary to the number of peaks, meaning that if the threshold isn't met and the number of peaks selected is under n, it will put those below the threshold to reach n. To change this set the forceThreshold parameter to True.  \n",
    "To disable thr threshold set it to 0 (default value).  \n",
    "The treshold is treated as absolute value, so negative values don't matter.  \n",
    "This method provides the remaining peaks as a test set, and the size can be specified with the test_n parameter. If left at 0 it will return all the discarded data for testing.  \n",
    "The randomTestSamples parameter allows to sample randomly from the remaining data, otherwise they're provided ordered by least uncerteinty value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectBestPeaks(data, n=10, threshold=0, forceThreshold=False, test_n=0, randomTestSamples=True, debug=False):\n",
    "    if n <=0:\n",
    "        return None\n",
    "    result = data.copy()\n",
    "    result[\"absolute_error\"] =  abs(data[\"error\"])\n",
    "    result = result.sort_values(by=['absolute_error'], ascending=True) #specified ascending parameter for flexibility\n",
    "    diff = result\n",
    "    test_n = abs(test_n)\n",
    "    \n",
    "    if debug:    \n",
    "        print(result)\n",
    "    \n",
    "    if threshold:\n",
    "        threshold = abs(threshold)\n",
    "        filtered = result[result['absolute_error']<=threshold]\n",
    "    \n",
    "    if len(result) > n:\n",
    "        if threshold:\n",
    "            if len(filtered) > n:\n",
    "                result = filtered[0:n]\n",
    "            else:\n",
    "                if forceThreshold:\n",
    "                    result = filtered\n",
    "                else:\n",
    "                    result = result[0:n]\n",
    "        else:\n",
    "            result = result[0:n]\n",
    "    else:\n",
    "        if forceThreshold and threshold:\n",
    "            result = filtered\n",
    "        else:\n",
    "            x = max(len(result), n)\n",
    "            result = result[0:x]\n",
    "            \n",
    "    diff = pd.concat([result,diff]).drop_duplicates(keep=False)\n",
    "    if test_n != 0:\n",
    "        test_n = min(test_n, len(diff))\n",
    "        if randomTestSamples:\n",
    "            diff = diff.sample(test_n)\n",
    "        else:\n",
    "            diff = diff[0:test_n]\n",
    "    return result, diff\n",
    "\n",
    "a, b = selectBestPeaks(peak_informations[0], 5, 1.397722e-06, False, 10, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test first peak selection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "\n",
    "train_information = []\n",
    "# Single degree models\n",
    "for n in [10, 20, 30, 40, 50, 100]:\n",
    "    for max_degree in range(1, 11):\n",
    "        for single_degree in [True, False]:\n",
    "            for weight in [1, None]:\n",
    "                min_degree = 1\n",
    "                if single_degree:\n",
    "                    min_degree = max_degree\n",
    "                mses = []\n",
    "                for peak_information in peak_informations:\n",
    "                    peak_information = peak_information[peak_information[\"error\"].notnull()]\n",
    "                    \n",
    "                    train, test = selectBestPeaks(peak_information, n=n, test_n=50)\n",
    "                    model = MultiPolyModel(list(range(min_degree, max_degree + 1)), weights = weight)\n",
    "                    model.fit(train[\"observed_mz\"].values, train[\"error\"].values*-1)\n",
    "                    preds = model.predict(test[\"observed_mz\"].values)\n",
    "                    mse = mean_squared_error(test[\"error\"].values*-1, preds)\n",
    "                    mses.append(mse)\n",
    "                train_information.append({\n",
    "                    \"min_degree\": min_degree,\n",
    "                    \"max_degree\": max_degree,\n",
    "                    \"weight\": weight,\n",
    "                    \"n\": n,\n",
    "                    \"mean_mse\": np.mean(mses),\n",
    "                    \"std_mse\": np.std(mses),\n",
    "                    \"min_mse\": np.min(mses),\n",
    "                    \"max_mse\": np.max(mses)\n",
    "                })\n",
    "        \n",
    "pd.DataFrame(train_information).sort_values(by=['mean_mse', \"std_mse\", \"max_mse\"], ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
