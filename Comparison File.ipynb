{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dramatic-yellow",
   "metadata": {},
   "source": [
    "# Testing different methods and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worldwide-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.optimize import leastsq\n",
    "from pyteomics import mass\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-scope",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "junior-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRoot = \"C:/Users/Antonio/Google Drive/Orbitrap project/Data/#2-#3 small deviation file/\"\n",
    "peaklistRoot = \"C:/Users/Antonio/Desktop/Esercizi prog/Data science project/peak list/\"\n",
    "data1 = [dataRoot + \"/#2/1 min\", dataRoot + \"/#2/2 min\"]\n",
    "data2 = [dataRoot + \"/#3/1 min\", dataRoot + \"/#3/2 min\"]\n",
    "peak_list_file = peaklistRoot + \"peaklist_1e5_background.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-small",
   "metadata": {},
   "source": [
    "### Jokinen Peak Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSpectraJokinen(path):\n",
    "    spectrum_data_files = []\n",
    "    spectra = []\n",
    "\n",
    "    # Get data files\n",
    "    for file in [f for f in listdir(path) if isfile(join(path, f))]:\n",
    "        path_to_file = join(path, file)\n",
    "        #print(path_to_file)\n",
    "        if file.endswith(\".csv\"):\n",
    "            spectrum_data_files.append(path_to_file)\n",
    "\n",
    "    # Read the spectrum files\n",
    "    for file in spectrum_data_files:\n",
    "        spectra.append(pd.read_csv(file))\n",
    "    \n",
    "    return spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra1 = [getSpectraJokinen(data1[0]), getSpectraJokinen(data1[1])]\n",
    "spectra2 = [getSpectraJokinen(data2[0]), getSpectraJokinen(data2[1])]\n",
    "\n",
    "peak_list = dict(pd.read_csv(peak_list_file).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peak_indices(data):\n",
    "    '''\n",
    "    Logic is same as in Koli's implementation. Tested that produces same results with spectrums in\n",
    "    \"data/CI-orbi_20201117165601 folder (first smal deviation file)/1 min\" folder.\n",
    "    '''\n",
    "    peak_start_indices = [0] #Initialize with first start index\n",
    "    peak_end_indices = []\n",
    "    for i, row in data.iterrows():\n",
    "        if i == 0:\n",
    "            continue;\n",
    "        if data.iloc[i][\"intensity\"] == 0.0 and data.iloc[i - 1][\"intensity\"] == 0.0:\n",
    "            peak_start_indices.append(i)\n",
    "            peak_end_indices.append(i - 1)\n",
    "    peak_end_indices.append(len(data) - 1) #Finalize with last end index\n",
    "    return np.column_stack((peak_start_indices, peak_end_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_resolution = 280000\n",
    "\n",
    "# x: data; a: height; x0: position; c: sigma or width\n",
    "gauss  = lambda x, a, mu, sigma: a*np.exp(-(x-mu)**2/(2*sigma**2))\n",
    "\n",
    "def fit_gaussian(peak, resolution=default_resolution, show=False):\n",
    "    '''\n",
    "    returns: a: 'height'; mu: 'position'; sigma: 'width'\n",
    "    '''\n",
    "    mu = np.average(peak[\"mz\"], weights=peak[\"intensity\"])\n",
    "    sigma = mu/(resolution*2*np.sqrt(2*np.log(2)))\n",
    "    errfunc  = lambda p, x, y: (y - gauss(x, p[0], p[1], sigma))\n",
    "    init  = [peak[\"intensity\"].max(), mu]\n",
    "    out = leastsq(errfunc, init, args=(peak[\"mz\"], peak[\"intensity\"]))\n",
    "    c = out[0]\n",
    "    if show:\n",
    "        x = np.linspace(peak[\"mz\"].min(), peak[\"mz\"].max(), 1000)\n",
    "        fig, ax = plt.subplots(figsize=(8,6))\n",
    "        ax.ticklabel_format(useOffset=False)\n",
    "        plt.plot(peak[\"mz\"], peak[\"intensity\"], \"b\")\n",
    "        plt.plot(x, gauss(x, c[0], c[1], sigma), \"g\")\n",
    "        plt.plot((c[1],c[1]), (0.0,c[0]), \"g\", linestyle=\"--\")\n",
    "    return c[0], c[1], sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPeaks(spectra):\n",
    "    peak_informations = []\n",
    "    for spectrum in spectra:\n",
    "        peak_information = []\n",
    "        for start, end in find_peak_indices(spectrum):\n",
    "            data = spectrum.iloc[start:(end+1)]\n",
    "            max_intensity = data[\"intensity\"].max()\n",
    "            width = data[\"mz\"].max() - data[\"mz\"].min()\n",
    "            average_mz = np.average(data[\"mz\"], weights=data[\"intensity\"])\n",
    "            peak_information.append({\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"max_intensity\": max_intensity,\n",
    "                \"average_mz\": average_mz,\n",
    "                \"width\": width\n",
    "            })\n",
    "        peak_informations.append(pd.DataFrame(peak_information))\n",
    "    return peak_informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks1 = [getPeaks(spectra1[0]),getPeaks(spectra1[1])]\n",
    "peaks2 = [getPeaks(spectra2[0]), getPeaks(spectra2[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatePeaks(spectra, peaks):\n",
    "    # Add 'observed' column\n",
    "    for spectrum, peak_information in zip(spectra, peaks):\n",
    "        fitted_means = []\n",
    "        for i, peak in peak_information.iterrows():\n",
    "            data = spectrum.iloc[int(peak[\"start\"]):int(peak[\"end\"] + 1)]\n",
    "            a, mu, sigma = fit_gaussian(data)\n",
    "            fitted_means.append(mu)\n",
    "        peak_information[\"observed_mz\"] = fitted_means\n",
    "    return peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = evaluatePeaks(spectra1[0], peaks1[0])\n",
    "b = evaluatePeaks(spectra1[0], peaks1[1])\n",
    "c = evaluatePeaks(spectra2[1], peaks2[0])\n",
    "d = evaluatePeaks(spectra2[1], peaks2[1])\n",
    "ePeaks1 = [a,b]\n",
    "ePeaks2 = [c,d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_peaks(peak_information, peak_list, theta = 0.01):\n",
    "    peak_information[\"formula\"] = None # For strings\n",
    "    peak_information[\"formula_mz\"] = np.nan # For floats\n",
    "    for key in peak_list.keys():\n",
    "        formula = key\n",
    "        true_mz = peak_list[key]\n",
    "        closest = peak_information[np.abs(peak_information[\"observed_mz\"] - true_mz) < theta]\n",
    "        if len(closest) > 0:\n",
    "            closest = closest.iloc[(closest['observed_mz'] - true_mz).abs().argsort()[:1]]\n",
    "            #print(\"True: {}; Observed: {}; Index: {}\".format(true_mz, closest[\"observed\"].values[0], closest.index.values[0]))\n",
    "            peak_information.at[closest.index.values[0], \"formula\"] = formula\n",
    "            peak_information.at[closest.index.values[0], \"formula_mz\"] = true_mz\n",
    "    peak_information[\"error\"] =  peak_information[\"formula_mz\"] - peak_information[\"observed_mz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "completePeaks1 = [identify_peaks(ePeaks1[0], peak_list),identify_peaks(ePeaks1[1], peak_list)]\n",
    "completePeaks2 = [identify_peaks(ePeaks2[0], peak_list),identify_peaks(ePeaks2[1], peak_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-collector",
   "metadata": {},
   "source": [
    "### Koli Peak Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "robust-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Find indices of the different peaks in the data\n",
    "def findPeakIndices(data):\n",
    "    n = int(round(sum(data[:,1] == 0)/2)+1)\n",
    "    indices = np.zeros([n,2], dtype=\"uint32\")\n",
    "    a = False\n",
    "    ii = 0\n",
    "    for i in range(len(data[:,1])-1):\n",
    "        if data[i,1] == 0 and a:\n",
    "            indices[ii, 1] = i - 1\n",
    "            if ii != n:\n",
    "                indices[ii+1, 0] = i\n",
    "            ii += 1\n",
    "        a = data[i,1] == 0\n",
    "    last = n-sum(indices[:,0]>=indices[:,1])\n",
    "    indices = indices[0:last+1,:]\n",
    "    indices[last, 1] = len(data[:,0]) - 1\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "superior-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import leastsq\n",
    "import numpy as np\n",
    "\n",
    "# Get one mean by fitting a Gaussian distribution to the data\n",
    "# with resolution of 280000\n",
    "def getMean(peak):\n",
    "    mu = np.average(peak[:,0], weights=peak[:,1])\n",
    "    sigma = mu/(280000*2*np.sqrt(2*np.log(2)))\n",
    "    fitfunc  = lambda p, x: p[0]*np.exp(-0.5*((x-p[1])/sigma)**2)\n",
    "    errfunc  = lambda p, x, y: (y - fitfunc(p, x))\n",
    "    init  = [1.0, mu]\n",
    "    out = leastsq(errfunc, init, args=(peak[:,0], peak[:,1]))\n",
    "    c = out[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "falling-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "import scipy.signal as scs\n",
    "\n",
    "# Calculate mean square error for the model\n",
    "def MSE(x, y, a, mu, sigma):\n",
    "    norm = lambda a, mu, x: a*np.exp(-0.5*((x-mu)/sigma)**2)\n",
    "    error = 0\n",
    "    for i in range(len(y)):\n",
    "        error = error + (y[i] - np.sum(norm(a,mu,x[i])))**2\n",
    "    return error/len(y)\n",
    "\n",
    "def errorFunction(x, y, sigma):\n",
    "    return lambda params: MSE(x, y, params[::2], params[1::2], sigma)\n",
    "\n",
    "# Function used to group values that are too close (abs(mu[n]-mu[n+1])<th) to each other together\n",
    "def group(mu, th):\n",
    "    groups = []\n",
    "    unassigned = np.linspace(0,len(mu)-1,len(mu)).astype(int)\n",
    "    while len(unassigned) > 0:\n",
    "        dist = abs(np.array(mu[unassigned]) - mu[unassigned[0]])\n",
    "        group = unassigned[dist < th]\n",
    "        unassigned = unassigned[dist >= th]\n",
    "        groups += [group]\n",
    "    return groups\n",
    "\n",
    "\n",
    "\n",
    "# Get one or more means depending on if there are multiple peaks in the data\n",
    "# by fitting one or more Gaussian distributions to the data using resolution of 280000 \n",
    "def getMeans(peaks):\n",
    "    ind = scs.find_peaks(peaks[:,1], max(peaks[:,1])/20, prominence=max(peaks[:,1])/10)[0]\n",
    "    mu = peaks[ind,0]\n",
    "    n = len(mu)\n",
    "    sigma = mu/(280000*2*np.sqrt(2*np.log(2)))\n",
    "    \n",
    "    init  = []\n",
    "    for i in range(len(mu)):\n",
    "        init += [np.mean(peaks[:,1]), mu[i]]\n",
    "    \n",
    "    # Fit n Gaussian distributions to the data\n",
    "    f = errorFunction(peaks[:,0], peaks[:,1], sigma)\n",
    "    params = minimize(f, init, method='BFGS').x\n",
    "    \n",
    "    a = params[::2]\n",
    "    mu = params[1::2]\n",
    "    \n",
    "    # Group fitted distributions that are too close to each other together\n",
    "    # and combine them\n",
    "    groups = group(mu, 1e-5)\n",
    "    a2 = np.zeros(len(groups))\n",
    "    mu2 = np.zeros(len(groups))\n",
    "    sigma2 = np.zeros(len(groups))\n",
    "    for i in range(len(groups)):\n",
    "        mu2[i] = np.mean(mu[groups[i]])\n",
    "        a2[i] = np.sum(a[groups[i]])\n",
    "        sigma2[i] = np.mean(sigma[groups[i]])\n",
    "    return mu2, a2, sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "anonymous-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get all peak means for the data\n",
    "def getAllMeans(peaks):\n",
    "    indices = findPeakIndices(peaks)\n",
    "    n = len(indices[:,0])\n",
    "    means = []\n",
    "    aa = []\n",
    "    sigmas = []\n",
    "    peaki = []\n",
    "    for i in range(n):\n",
    "        peak = peaks[indices[i,0]:indices[i,1]+1,:]\n",
    "        mu, a, sigma = getMeans(peak)\n",
    "        for ii in range(len(a)):\n",
    "            means += [mu[ii]]\n",
    "            aa += [a[ii]]\n",
    "            sigmas += [sigma[ii]]\n",
    "            peaki += [i]\n",
    "    return means, aa, sigmas, peaki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "packed-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Identify peaks and return a data frame of peaks and their properties\n",
    "def identifyPeaks(data, peaklist, th):\n",
    "    peaklist['observed'] = np.zeros(len(peaklist['mz']))\n",
    "    peaklist['a'] = np.zeros(len(peaklist['mz']))\n",
    "    peaklist['sigma'] = np.zeros(len(peaklist['mz']))\n",
    "    peaklist['peak'] = np.zeros(len(peaklist['mz']))\n",
    "    peakMeans, a, sigmas, ind = getAllMeans(data)\n",
    "    trueValue = peaklist['mz'].to_numpy()\n",
    "    for i in range(len(trueValue)):\n",
    "        if min(abs(peakMeans - trueValue[i])) < th:\n",
    "            j = np.argmin(abs(peakMeans - trueValue[i]))\n",
    "            peaklist.iloc[i,2] = peakMeans[j]\n",
    "            peaklist.iloc[i,3] = a[j]\n",
    "            peaklist.iloc[i,4] = sigmas[j]\n",
    "            peaklist.iloc[i,5] = ind[j]\n",
    "    unidentified = np.linspace(0, len(trueValue)-1, len(trueValue))\n",
    "    unidentified = unidentified[peaklist['observed'].to_numpy() == 0]\n",
    "    return peaklist.drop(unidentified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "shared-narrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def readFile(fileName):\n",
    "    return pd.read_csv(fileName).iloc[1:,:2].to_numpy().astype(\"float64\")\n",
    "\n",
    "def getSpectraKoli(path):\n",
    "    spectrum_data_files = []\n",
    "    spectra = []\n",
    "\n",
    "    # Get data files\n",
    "    for file in [f for f in listdir(path) if isfile(join(path, f))]:\n",
    "        path_to_file = join(path, file)\n",
    "        #print(path_to_file)\n",
    "        if file.endswith(\".csv\"):\n",
    "            spectrum_data_files.append(path_to_file)\n",
    "\n",
    "    # Read the spectrum files\n",
    "    for file in spectrum_data_files:\n",
    "        spectra.append(readFile(file))\n",
    "    \n",
    "    return spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "parliamentary-florist",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra1 = [getSpectraKoli(data1[0]), getSpectraKoli(data1[1])]\n",
    "spectra2 = [getSpectraKoli(data2[0]), getSpectraKoli(data2[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "everyday-programmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\orbitool\\lib\\site-packages\\scipy\\optimize\\optimize.py:1166: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (np.dot(yk, sk))\n"
     ]
    }
   ],
   "source": [
    "peaklist = pd.read_csv(peak_list_file)\n",
    "\n",
    "a = []\n",
    "for s in spectra1[0]:\n",
    "    a.append(identifyPeaks(s, peaklist, 1e-4))\n",
    "\n",
    "b = []\n",
    "for s in spectra1[1]:\n",
    "    b.append(identifyPeaks(s, peaklist, 1e-4))\n",
    "    \n",
    "completePeaks1 = [a,b]\n",
    "    \n",
    "c = []\n",
    "for s in spectra2[0]:\n",
    "    c.append(identifyPeaks(s, peaklist, 1e-4))\n",
    "d = []\n",
    "for s in spectra2[1]:\n",
    "    d.append(identifyPeaks(s, peaklist, 1e-4))\n",
    "completePeaks2 = [c,d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tropical-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in completePeaks1:\n",
    "    for s in d:\n",
    "        s[\"error\"] =  s[\"mz\"] - s[\"observed\"]\n",
    "for d in completePeaks2:\n",
    "    for s in d:\n",
    "        s[\"error\"] =  s[\"mz\"] - s[\"observed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "roman-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def saveProcessedData(dfArray, folder, path=\"./\"):\n",
    "    if not os.path.exists(path+folder):\n",
    "        os.makedirs(path+folder)\n",
    "    for i in range(len(dfArray)):\n",
    "        name = \"spectrum_\"+str(i).zfill(3)+\".csv\"\n",
    "        dfArray[i].to_csv(path+folder+os.path.sep+name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "polish-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveProcessedData(completePeaks1[0], \"Example_Inputs/data1/1min\")\n",
    "saveProcessedData(completePeaks1[1], \"Example_Inputs/data1/2min\")\n",
    "saveProcessedData(completePeaks2[0], \"Example_Inputs/data2/1min\")\n",
    "saveProcessedData(completePeaks2[1], \"Example_Inputs/data2/2min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-wisdom",
   "metadata": {},
   "source": [
    "## Selection method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "literary-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectBestPeaks(data, n=10, threshold=0, forceThreshold=False, test_n=0, randomTestSamples=True, debug=False):\n",
    "    if n <=0:\n",
    "        return None\n",
    "    result = data.copy()\n",
    "    result[\"absolute_error\"] =  abs(data[\"error\"])\n",
    "    result = result.sort_values(by=['absolute_error'], ascending=True) #specified ascending parameter for flexibility\n",
    "    diff = result\n",
    "    test_n = abs(test_n)\n",
    "    \n",
    "    if debug:    \n",
    "        print(result)\n",
    "    \n",
    "    if threshold:\n",
    "        threshold = abs(threshold)\n",
    "        filtered = result[result['absolute_error']<=threshold]\n",
    "    \n",
    "    if len(result) > n:\n",
    "        if threshold:\n",
    "            if len(filtered) > n:\n",
    "                result = filtered[0:n]\n",
    "            else:\n",
    "                if forceThreshold:\n",
    "                    result = filtered\n",
    "                else:\n",
    "                    result = result[0:n]\n",
    "        else:\n",
    "            result = result[0:n]\n",
    "    else:\n",
    "        if forceThreshold and threshold:\n",
    "            result = filtered\n",
    "        else:\n",
    "            x = max(len(result), n)\n",
    "            result = result[0:x]\n",
    "            \n",
    "    diff = pd.concat([result,diff]).drop_duplicates(keep=False)\n",
    "    if test_n != 0:\n",
    "        test_n = min(test_n, len(diff))\n",
    "        if randomTestSamples:\n",
    "            diff = diff.sample(test_n)\n",
    "        else:\n",
    "            diff = diff[0:test_n]\n",
    "    return result, diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-animation",
   "metadata": {},
   "source": [
    "## Selection method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "several-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose n peaks with the smallest error\n",
    "def selectPeaksMinimumError(peaks, n):\n",
    "    errors = abs(peaks.iloc[:,1]-peaks.iloc[:,2])\n",
    "    ind = np.argpartition(errors, n)\n",
    "    return peaks.iloc[ind[:n],:], peaks.iloc[ind[n:],:]\n",
    "\n",
    "# Choose n peaks with error closest to the mean error\n",
    "def selectPeaksMeanError(peaks, n):\n",
    "    errors = abs(peaks.iloc[:,1]-peaks.iloc[:,2])\n",
    "    ind = np.argpartition(abs(errors-np.mean(errors)), n)\n",
    "    return peaks.iloc[ind[:n],:], peaks.iloc[ind[n:],:]\n",
    "\n",
    "# Choose n peaks with error closest to the median error\n",
    "def selectPeaksMedianError(peaks, n):\n",
    "    errors = abs(peaks.iloc[:,1]-peaks.iloc[:,2])\n",
    "    ind = np.argpartition(abs(errors-np.median(errors)), n)\n",
    "    return peaks.iloc[ind[:n],:], peaks.iloc[ind[n:],:]\n",
    "\n",
    "# Splits the peaks into k partitions and selects n peaks from each partition\n",
    "# so it results in n*k peaks for training and the rest of the peaks for testing\n",
    "def selectPeaks(peaklist, n, k, select=selectPeaksMinimumError):\n",
    "    n_peaks = len(peaklist.iloc[:,0])\n",
    "    partition_size = int((n_peaks - (n_peaks % k)) / k)\n",
    "    test = pd.DataFrame(columns=[\"formula\",\"mz\",\"observed\",\"a\",\"sigma\",\"peak\"])\n",
    "    train = pd.DataFrame(columns=[\"formula\",\"mz\",\"observed\",\"a\",\"sigma\",\"peak\"])\n",
    "    for i in range(k):\n",
    "        if i < k-1:\n",
    "            partition = peaklist.iloc[i*partition_size:(i+1)*partition_size,:]\n",
    "        else:\n",
    "            partition = peaklist.iloc[i*partition_size:,:]\n",
    "        tr, ts = select(partition, n)\n",
    "        test = test.append(ts)\n",
    "        train = train.append(tr)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "magnetic-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineObservations(peaklists):\n",
    "    peaklist = peaklists[0][[\"formula\", \"mz\", \"observed\"]].set_index(\"formula\")\n",
    "    for i in range(len(peaklists)-1):\n",
    "        peaklist = peaklist.join(peaklists[i+1][[\"formula\", \"observed\"]].set_index(\"formula\"), on=\"formula\", rsuffix='_'+str(i+1))\n",
    "    return peaklist\n",
    "\n",
    "def combinePeaklists(peaklists):\n",
    "    peaklist = peaklists[0].set_index(\"formula\")\n",
    "    for i in range(len(peaklists)-1):\n",
    "        peaklist = peaklist.join(peaklists[i+1].drop(\"mz\", axis=1).set_index(\"formula\"), on=\"formula\", rsuffix='_'+str(i+1))\n",
    "    return peaklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-polymer",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "above-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_poly_reg_model(x, y, degree, show = False, ax = None, color=\"r\"):\n",
    "    poly_features = PolynomialFeatures(degree)\n",
    "    model = make_pipeline(poly_features, LinearRegression()).fit(x.reshape(-1, 1), y)\n",
    "    if show:\n",
    "        print(\"Coefficients: {}\".format(model.steps[1][1].coef_[1:]), \"Independent: {}\".format(model.steps[1][1].intercept_))\n",
    "        preds = model.predict(x.reshape(-1, 1))\n",
    "        sns.set_theme(style=\"whitegrid\")\n",
    "        if not ax:\n",
    "            fig, ax = plt.subplots(figsize=(8,6))\n",
    "        sns.scatterplot(x=x, y=y, ax=ax, color=color)\n",
    "        plt.plot(x, preds, linestyle=\"-\", color=color)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-alaska",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "pursuant-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def readProcessedData(path):\n",
    "    array = []\n",
    "    for f in listdir(path):\n",
    "        array.append(pd.read_csv(path+os.path.sep+f))\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "herbal-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./Example_Inputs/\"\n",
    "\n",
    "a = readProcessedData(root + \"data1/1min\")\n",
    "b = readProcessedData(root + \"data1/2min\")\n",
    "completePeaks1 = [a,b]\n",
    "\n",
    "c = readProcessedData(root + \"data2/1min\")\n",
    "d = readProcessedData(root + \"data2/2min\")\n",
    "completePeaks2 = [c,d]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-pavilion",
   "metadata": {},
   "source": [
    "The following function tests a linear model with 5 degrees using the first selection method (aka selectBestPeaks).  \n",
    "The parameters are an array of dataframes conataining all the informations of the peaks (post analysis), and an array of numbers of peaks for the training selection.  \n",
    "The output is an array of the results in Mean Squared Error (mean, min, max), and std deviation along with the test parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "popular-identifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModelBestPeaks(completePeaks, train_n=[10, 20, 30, 40, 50]):\n",
    "    train_information = []\n",
    "    for n in train_n:\n",
    "        for degree in [1, 2,3,4,5]:\n",
    "            mses = []\n",
    "            for peak_information in completePeaks:\n",
    "                peak_information = peak_information[peak_information[\"error\"].notnull()]\n",
    "                n_data = len(peak_information)\n",
    "                train, test = selectBestPeaks(peak_information, n=n, test_n=15)\n",
    "                if len(test) > 0:\n",
    "                    model = fit_poly_reg_model(train[\"observed\"].values, train[\"error\"].values*-1, degree)\n",
    "                    preds = model.predict(test[\"observed\"].values.reshape(-1, 1))\n",
    "                    mse = mean_squared_error(test[\"error\"].values*-1, preds)\n",
    "                    mses.append(mse)\n",
    "            train_information.append({\n",
    "                \"degree\": degree,\n",
    "                \"n\": n,\n",
    "                \"mean_mse\": np.mean(mses),\n",
    "                \"min_mse\": np.min(mses),\n",
    "                \"max_mse\": np.max(mses),\n",
    "                \"std_mse\": np.std(mses)\n",
    "            })\n",
    "#pd.DataFrame(train_information).sort_values(by=['mean_mse', \"std_mse\"], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-serial",
   "metadata": {},
   "source": [
    "The following function tests a linear model with 5 degrees using the second selection method (aka partition method).  \n",
    "The parameters are an array of dataframes conataining all the informations of the peaks (post analysis), the start and for the iteration over the partitions, and the start and end for the iteration over the peaks per partition (abbreviated as *ppp*).  \n",
    "The output is an array of the results in Mean Squared Error (mean, min, max), and std deviation along with the test parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "tropical-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModelPartitions(completePeaks, partitions_start=2, partitions_end=5, ppp_start=2, ppp_end=5):\n",
    "    train_information = []\n",
    "    for part in range(partitions_start, partitions_end):\n",
    "        for ppp in  range(ppp_start, ppp_end):\n",
    "            for degree in [1, 2,3,4,5]:\n",
    "                mses = []\n",
    "                for peak_information in completePeaks:\n",
    "                    peak_information = peak_information[peak_information[\"error\"].notnull()]\n",
    "                    n_data = len(peak_information)\n",
    "                    train, test = selectPeaks(peak_information, ppp, part)\n",
    "                    if len(test) > 0:\n",
    "                        model = fit_poly_reg_model(train[\"observed\"].values, train[\"error\"].values*-1, degree)\n",
    "                        preds = model.predict(test[\"observed\"].values.reshape(-1, 1))\n",
    "                        mse = mean_squared_error(test[\"error\"].values*-1, preds)\n",
    "                        mses.append(mse)\n",
    "                train_information.append({\n",
    "                    \"degree\": degree,\n",
    "                    \"partitions\": part,\n",
    "                    \"n\": ppp,\n",
    "                    \"mean_mse\": np.mean(mses),\n",
    "                    \"min_mse\": np.min(mses),\n",
    "                    \"max_mse\": np.max(mses),\n",
    "                    \"std_mse\": np.std(mses)\n",
    "                })\n",
    "#pd.DataFrame(train_information).sort_values(by=['mean_mse', \"std_mse\"], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for gathering the test results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-mumbai",
   "metadata": {},
   "source": [
    "## Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-aggregate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
